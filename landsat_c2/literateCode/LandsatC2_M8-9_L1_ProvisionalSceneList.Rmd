---
title: "Pull Landsat Collection 2 L1 stacks for Missions 8 and 9 - provisional"
author: "B Steele"
format: html
jupyter: python3
---

```{r setup}
library(nhdplusTools)
library(tidyverse)
library(sf)

#point to the directory where your location information is stored
#make sure this path ends with a '/'
data_dir = '/Users/steeleb/Documents/GitHub/sky-loch-explore/data/'
```

## *Purpose*

This script grabs the Landsat Product ID from the Level 1 product in order to populate a list of Scenes to submit for the Provisional Aquatic Reflectance product.

## *Requirements*

This code requires the user to run some terminal commands. You should be able to use any zsh terminal to complete these commands. You will also need a [Google Earth Engine account](https://earthengine.google.com/signup/), and then you will need to [download, install, and initialize gcloud](https://cloud.google.com/sdk/docs/install) for this to function.

## *Prepare!*

### Set up your `reticulate` virtual environment

This step will set up and activate the Python virtual environment using `reticulate` and install the required Python packages. If you get errors at this step, make sure that you have set up R Markdown to run code chunks relative to the project file ('landsat_c2.Rproj'). You may have to adjust the directories if you are working in a Rproj in a different relative folder path. Refer to the 'helps' section of the ROSS_RS_mini_tools repository for common troubleshooting solutions.

```{r}
py_env_dir = getwd() #this is where the env folder will be saved
source(file.path(py_env_dir, 'literateCode', 'pySetup.R'))
```

### Import python modules.

These are the modules that will be used in the script.

```{python}
import time
import ee
import os
import fiona
from pandas import read_csv
from datetime import date

#pull directory path from r to python
dataDir = r.data_dir
```

### Authenticate earth engine.

At the moment, 'ee.Authenticate()' is not working in Qmd/Rmd, to authenticate manually, go to your command line interpreter or the `zsh` terminal in RStudio (`BASH` terminals will not work) and execute:

`earthengine authenticate`

### Initialize earth engine.

```{python}
ee.Initialize()
```

### Load in location data

*Read in lat/lon file and create an EE asset. Location file must use the column names 'Latitude' and 'Longitude', otherwise make sure you rename them before running the function.*

```{r}
#point to file - must contin the parameters Latitude, Longitude, comid, and name
locs = read.csv(file.path(data_dir, 'location_lat_longs_sky_loch.csv'))

#rename to required cols Latitude, Longitude, id, name
locs = locs %>% 
  rowid_to_column() %>% 
  rename(name = location,
         id = rowid)

#give this a short project name (for file naming conventions)
proj = 'sky_loch'

#and specify a folder name for the landsat stacks to get dumped into in your Google Drive. The script will create the folder if it does not exist. Note, if you have a Drive folder name that matches your 'proj_folder' name, all of your output will save there regardless of capitalization.
proj_folder = 'sky_loch'
```

### Specify time period of interest

As written below, this script will pull all historical Landsat 8 and 9 images. If you wish to focus the script to a shortened time period, you may adjust these. Landsat 8 was deployed in 2013.

```{r}
start_date_89 = '2013-01-01'
end_date_89 = as.character(Sys.Date())
```

------------------------------------------------------------------------

## **You shouldn't have to alter any code after this point.**

------------------------------------------------------------------------

### *Prepare your site data*

Transform the site location .csv into a GEE feature

```{python}
def csv_to_eeFeat(df):
  features=[]
  for i in range(df.shape[0]):
    x,y = df.Longitude[i],df.Latitude[i]
    latlong =[x,y]
    loc_properties = {'system:index':str(df.id[i]), 'name':df.name[i], 'id':str(df.id[i])}
    g=ee.Geometry.Point(latlong) 
    feature = ee.Feature(g, loc_properties)
    features.append(feature)
  ee_object = ee.FeatureCollection(features)
  return ee_object

locs_feature = csv_to_eeFeat(r.locs)  

#check to make sure everything showed up.
locs_feature.getInfo()
```

## Load WRS tiles

Grab WRS tiles (these are the 'path' and 'rows' that Landsat operates on) in descending (daytime) mode for CONUS. We'll use the path-row information to subset data later on to prevent GEE from hanging due to information overload.

```{python}
wrs = ee.FeatureCollection('users/sntopp/wrs2_asc_desc')\
    .filterBounds(locs_feature) #grab only wrs overlap with dp
wrs = wrs.filterMetadata('MODE', 'equals', 'D') #only grab the descending (daytime) path
    
pr = wrs.aggregate_array('PR').getInfo() #create PathRow list
```

## *Load in Landsat Collections*

Grab all Landsat Collection 2 image collections, apply scaling factors, and harmonize band names and definitions

#### get image collections

As written, this script only removes scenes with 95% cloud cover. If you're processing data over a very large area (regions of the United states or larger), you may wish to increase this initial filter to decrease processing time. The default for Aquasat v1 was 75% cover because in Collection 1 there were persistent artefacts of cloud cover in the RS data.

```{python}
#grab images and apply scaling factors
l8 = (ee.ImageCollection('LANDSAT/LC08/C02/T1')
    .filter(ee.Filter.lt('CLOUD_COVER', 95))
    .filterDate(r.start_date_89, r.end_date_89))
l9 = (ee.ImageCollection('LANDSAT/LC09/C02/T1')
    .filter(ee.Filter.lt('CLOUD_COVER', 95))
    .filterDate(r.start_date_89, r.end_date_89))

# merge collections by image processing groups
ls89 = ee.ImageCollection(l8.merge(l9)).filterBounds(wrs)  
    
# do a reality check to see how many unique scenes are here. This can take a few seconds to run if it's a large area - I don't suggest this if the length of your WRS object is >5.
if len(pr) <= 5 :
  ls89_count = ls89.aggregate_count('LANDSAT_PRODUCT_ID').getInfo()
  print(ls89_count)

```

## *Load functions*

### General functions:

#### dpBuff: To buffer lat/longs

```{python}
## Buffer the lake sites
def dpBuff(i):
  return i.buffer(90) #doing a 90m buffer for general use

```

#### *removeGeo: Function to remove geometry from image collections*

```{python}
## Remove geometries
def removeGeo(i):
  return i.setGeometry(None)

```

## *Run the GEE functions.*

### Send the metatdata request to GEE

```{python}
for tiles in pr:
  tile = wrs.filterMetadata('PR', 'equals', tiles)

  ## get metadata ##
  meta_srname = r.proj+'_metadata_LS89_C2_LEVEL1_'+str(tiles)+'_v'+str(date.today())
  meta_dataOut = (ee.batch.Export.table.toDrive(collection = ls89,
                                          description = meta_srname,
                                          folder = r.proj_folder,
                                          fileFormat = 'csv'))
  
  #Send next task.                                        
  meta_dataOut.start()
  
  print('done with tile ' + str(tiles))
  
print('done with all tiles')
```

That's it! Your GEE tasks are now running [here](https://code.earthengine.google.com/tasks) and the output will show up in your Google Drive.
